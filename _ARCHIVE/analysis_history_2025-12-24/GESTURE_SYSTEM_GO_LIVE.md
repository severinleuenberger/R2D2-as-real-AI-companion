# ğŸš€ Gesture System - GO LIVE!

**Date:** December 17, 2025  
**Status:** READY FOR TESTING  
**Target:** Real-world gesture-to-speech integration

---

## ğŸ¯ What We're Testing

**Complete gesture-based speech control:**
1. ğŸ‘† **Index finger up** â†’ Start speech service â†’ ğŸ”Š Beep (16.mp3)
2. âœŠ **Fist** â†’ Stop speech service â†’ ğŸ”Š Beep (20.mp3)
3. â° **Auto-shutdown** â†’ Service stops after 5 min no presence â†’ ğŸ”Š Beep (20.mp3)

---

## âš¡ Quick Start

### **Option 1: Automated Test Script** (Recommended)

```bash
cd ~/dev/r2d2
./test_gesture_system_live.sh
```

This script will:
- âœ… Check all prerequisites
- âœ… Start required services
- âœ… Launch gesture intent node
- âœ… Provide step-by-step test instructions
- âœ… Monitor gesture events

### **Option 2: Manual Launch** (For Advanced Users)

**Terminal 1: Gesture Intent Node**
```bash
source /opt/ros/humble/setup.bash
source ~/dev/r2d2/ros2_ws/install/setup.bash
ros2 launch r2d2_gesture gesture_intent.launch.py \
    enabled:=true \
    audio_feedback_enabled:=true \
    auto_shutdown_enabled:=true
```

**Terminal 2: Speech Service** (if not auto-started)
```bash
bash ~/dev/r2d2/launch_ros2_speech.sh
```

**Terminal 3: Monitor Topics**
```bash
source /opt/ros/humble/setup.bash
source ~/dev/r2d2/ros2_ws/install/setup.bash
ros2 topic echo /r2d2/perception/gesture_event
```

---

## ğŸ§ª Test Procedure

### **Test 1: Face Recognition**
1. Stand in front of camera
2. **Expected:** LED turns RED
3. **Expected:** Terminal shows "Target person detected: severin"

---

### **Test 2: Start Speech with Gesture**

**Gesture: Index Finger Up** ğŸ‘†

1. Raise your INDEX FINGER pointing upward
2. Hold steady for 2-3 seconds
3. Keep other fingers closed/relaxed

**Expected Results:**
- âœ… Terminal shows: `Gesture detected: index_finger_up (confidence: 0.XX)`
- âœ… Terminal shows: `Starting speech session...`
- ğŸ”Š **Hear R2D2 beep** (sound 16.mp3)
- âœ… Speech service becomes active

**Verification:**
```bash
ros2 topic echo /r2d2/speech/session_status --once
# Should show: status: "active" or "connected"
```

---

### **Test 3: Use Speech Service**

1. Say something to R2D2
2. R2D2 should respond
3. Have a short conversation

**Expected:**
- âœ… Voice is transcribed
- âœ… R2D2 responds with audio

---

### **Test 4: Stop Speech with Gesture**

**Gesture: Fist** âœŠ

1. Make a tight FIST (all fingers closed)
2. Hold steady for 2-3 seconds
3. Keep fist closed

**Expected Results:**
- âœ… Terminal shows: `Gesture detected: fist (confidence: 0.XX)`
- âœ… Terminal shows: `Stopping speech session...`
- ğŸ”Š **Hear R2D2 beep** (sound 20.mp3)
- âœ… Speech service stops

**Verification:**
```bash
ros2 topic echo /r2d2/speech/session_status --once
# Should show: status: "inactive"
```

---

### **Test 5: Auto-Shutdown** (Optional - Takes 5+ minutes)

1. Start speech service with index finger gesture
2. **Walk away from camera**
3. LED turns BLUE (no person detected)
4. **Wait 5 minutes**

**Expected Results:**
- â° After 5 min: Terminal shows `No person presence for 300s. Auto-stopping speech service.`
- ğŸ”Š Hear R2D2 beep (sound 20.mp3)
- âœ… Speech service automatically stops

---

## ğŸ“Š Monitoring

### **Key Topics to Watch:**

**Person Status:**
```bash
ros2 topic echo /r2d2/audio/person_status
```
- `"red"` = You're recognized âœ…
- `"blue"` = No person / not recognized
- `"green"` = Face detected, not recognized

**Gesture Events:**
```bash
ros2 topic echo /r2d2/perception/gesture_event
```
- Shows detected gestures with confidence
- Only published when target person is recognized

**Speech Session:**
```bash
ros2 topic echo /r2d2/speech/session_status
```
- `"active"` / `"connected"` = Session running
- `"inactive"` / `"disconnected"` = Session stopped

---

## ğŸ”§ Configuration

### **Gesture Recognition Parameters:**

Located in: `/home/severin/dev/r2d2/ros2_ws/src/r2d2_bringup/launch/r2d2_camera_perception.launch.py`

```python
enable_gesture_recognition: true
gesture_recognition_model_path: /home/severin/dev/r2d2/data/gesture_recognition/models/severin_gesture_classifier.pkl
gesture_confidence_threshold: 0.7  # 70% confidence required
gesture_frame_skip: 5  # Process every 5th frame
```

### **Gesture Intent Parameters:**

Located in: `/home/severin/dev/r2d2/ros2_ws/src/r2d2_gesture/launch/gesture_intent.launch.py`

```python
enabled: true
cooldown_start_seconds: 5.0  # Prevent multiple starts
cooldown_stop_seconds: 3.0   # Prevent multiple stops
auto_shutdown_enabled: true
auto_shutdown_timeout_seconds: 300.0  # 5 minutes
audio_feedback_enabled: true
```

---

## ğŸ› Troubleshooting

### **Gestures Not Recognized**

**Symptom:** No gesture events even when making gestures

**Check:**
1. Is face recognized? (LED must be RED)
   ```bash
   ros2 topic echo /r2d2/audio/person_status
   ```
2. Is gesture recognition enabled?
   ```bash
   ros2 param get /image_listener enable_gesture_recognition
   ```
3. Does model exist?
   ```bash
   ls -lh /home/severin/dev/r2d2/data/gesture_recognition/models/severin_gesture_classifier.pkl
   ```

**Fix:**
- Make sure you're recognized (face detection RED LED)
- Hold gesture for 2-3 seconds
- Make gesture clear and distinct
- Check lighting

---

### **No Audio Feedback**

**Symptom:** Gestures work but no beeps

**Check:**
1. Audio enabled?
   ```bash
   ros2 param get /gesture_intent_node audio_feedback_enabled
   ```
2. Files exist?
   ```bash
   ls -lh ~/Voicy_R2-D2\ -\ 16.mp3
   ls -lh ~/Voicy_R2-D2\ -\ 20.mp3
   ```
3. Volume level?
   ```bash
   amixer get Master
   ```

**Fix:**
```bash
# Set volume
amixer set Master 70%

# Test playback
aplay ~/Voicy_R2-D2\ -\ 16.mp3
```

---

### **Speech Service Not Starting**

**Symptom:** Gesture recognized but service doesn't start

**Check:**
1. Is speech service running?
   ```bash
   ros2 service list | grep speech
   ```
2. Check gesture_intent_node logs in its terminal

**Fix:**
```bash
# Launch speech service manually
bash ~/dev/r2d2/launch_ros2_speech.sh
```

---

### **Service Already Running Error**

**Symptom:** `Service is already active (cooldown: X.X seconds)`

**Explanation:** This is intentional! Cooldown prevents accidental repeated triggers.

**Wait:** 5 seconds for start, 3 seconds for stop

---

## ğŸ“ˆ Expected Performance

### **Recognition Accuracy:**
- **Face Recognition:** >90% when well-lit, facing camera
- **Gesture Recognition:** >85% with clear gestures
- **Combined:** Gestures only trigger when face recognized âœ…

### **Latency:**
- **Gesture Detection:** ~1-2 seconds (with frame skip)
- **Service Start:** ~2-3 seconds
- **Audio Feedback:** Immediate after service change
- **Auto-Shutdown:** Triggers at exactly 5:00 minutes

### **Resource Usage:**
- **CPU:** +10-15% for gesture recognition (with frame skip)
- **Memory:** +100MB for MediaPipe
- **No impact on face recognition performance**

---

## âœ… Success Criteria

**Test is successful if:**
- âœ… Face recognition works (RED LED when you're in view)
- âœ… Index finger up starts speech service
- âœ… Fist stops speech service
- âœ… Hear beeps on start (16.mp3) and stop (20.mp3)
- âœ… Speech service actually works (you can talk to R2D2)
- âœ… Auto-shutdown triggers after 5 min no presence
- âœ… No crashes or errors in terminals
- âœ… System returns to normal after testing

---

## ğŸ‰ What's Working

**Implemented & Tested:**
- âœ… Gesture capture (training workflow)
- âœ… Gesture training (SVM classifier)
- âœ… Person entity management (SQLite registry)
- âœ… Gesture recognition in perception pipeline
- âœ… Gesture-to-speech intent mapping
- âœ… Audio feedback (R2D2 beeps)
- âœ… Auto-shutdown watchdog
- âœ… Cooldown prevention
- âœ… State-based gating (only when person recognized)

**Ready for Production:**
- âœ… Training helper script (safe service management)
- âœ… ROS2 integration (all nodes, topics, services)
- âœ… Documentation (architecture, training, usage)
- âœ… Error handling and logging

---

## ğŸš€ Let's Go!

**Run the test:**
```bash
cd ~/dev/r2d2
./test_gesture_system_live.sh
```

**Then follow the test procedure and have fun controlling R2D2 with gestures!** ğŸ¤–âœ¨

---

## ğŸ“ Notes for Future

**Next Steps (After Testing):**
1. Fine-tune gesture confidence thresholds based on real-world performance
2. Add more gestures if needed (peace sign, thumbs up, etc.)
3. Link persons to Google accounts (as planned)
4. Implement multi-person gesture recognition
5. Add gesture history/analytics

**Performance Tuning:**
- Adjust `gesture_frame_skip` (currently 5) for speed vs. CPU tradeoff
- Adjust `gesture_confidence_threshold` (currently 0.7) for accuracy vs. false positives
- Adjust cooldown timers if too restrictive

---

**System Status:** ğŸŸ¢ READY FOR LIVE TESTING

**Good luck with the test! The entire gesture system is now active and waiting for your commands!** ğŸ¯

