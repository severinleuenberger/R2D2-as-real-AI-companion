# R2D2 as a Real AI Companion

> Transform the iconic 1:2 DeAgostini R2-D2 into a fully autonomous AI companion robot with vision, voice, navigation, and personality‚Äîpowered by NVIDIA Jetson AGX Orin 64GB and ROS 2 Humble.

![R2D2 Full Robot](docs/photos/20251107_105518.jpg)

**Current Status:** Phase 1 Core System ‚Äî ~85% complete  
**Next Phase:** Speech & Language (STT + LLM + TTS)  
**Project Timeline:** 4 phases, 280-300 hours total (7 weeks full-time, 3-4 months part-time)

---

## üéØ Core Functional Requirements

The R2D2 project is designed to achieve the following functional capabilities:

### Communication & Conversation
- The robot must **listen to spoken input, convert speech to text, understand the user, generate a response, and speak it back** using text-to-speech
- The robot must **support natural, multi-turn conversation** without needing manual resets
- The robot must **allow direct verbal commands** such as "follow me," "go to the living room," "come here," or "look at this"
- The robot must **operate locally without requiring cloud services**, including offline STT, LLM, and TTS

### Perception & Recognition
- The robot must **recognize people, especially Severin, including detecting and identifying faces** through computer vision
- The robot must **track a person and optionally follow them** through the environment
- The robot must **recognize objects and obstacles** in front of it using camera and depth perception
- The robot must **detect rooms or general environment context** and understand where it is inside a home or office
- The robot must **react to events** in the environment, such as someone entering a room or calling its name
- The robot must **orient its camera** toward the person speaking or the object of interest

### Autonomous Navigation & Safety
- The robot must **autonomously navigate through indoor spaces, build a map, avoid obstacles, and move to requested locations**
- The robot must **maintain a safe driving speed and avoid collisions** with furniture, people, or pets
- The robot must **provide a heartbeat or "alive" signal** to indicate it is powered and operational

### Expression & Multi-Modal Interaction
- The robot must **respond with emotional or expressive sounds** similar to R2-D2 beeps
- The robot must **be able to play audio files**, including R2-D2 sound effects
- The robot must **combine perception, navigation, and conversation** so that it can interact socially and physically at the same time
---

## üöÄ Features (Current & Planned)

### Phase 1: Core System (Current) ‚úÖ
- [x] **Real-time Perception:** 30 FPS RGB camera stream, brightness metrics, face detection (90% accuracy)
- [x] **Face Recognition:** LBPH-based person identification (trained for multiple users)
- [x] **Hardware Integration:** OAK-D Lite depth camera, Jetson AGX Orin compute, ROS 2 infrastructure
- [x] **Professional Codebase:** Clean workspace structure, modular packages, parameter-driven configuration
- [x] **Comprehensive Docs:** Setup guides, integration patterns, operations checklist, architecture diagrams
- [x] **Power Button Control:** Shutdown control via Pin 32, boot/wake via J42 automation header ‚úÖ TESTED

### Phase 2: Speech & AI (Next) ‚è≥
- [ ] **Speech-to-Text:** Real-time audio input with wake-word detection ("Hey R2D2")
- [ ] **Language Model:** Local LLM inference (Llama 2 7B) for conversational responses
- [ ] **Text-to-Speech:** Real-time voice synthesis and playback
- [ ] **Context Awareness:** Vision-informed responses (greet by name, reference visual context)

### Phase 3: Navigation (Future) ‚è≥
- [ ] **SLAM Mapping:** Autonomous room mapping and localization
- [ ] **Autonomous Movement:** Differential drive control for 2-wheel locomotion
- [ ] **Obstacle Avoidance:** Real-time collision detection and path replanning
- [ ] **Room Navigation:** Go to named rooms, return to base, explore autonomously

### Phase 4: Memory & Personality (Future) ‚è≥
- [ ] **Conversation Memory:** Persistent history, context-aware responses
- [ ] **Learning & Adaptation:** Preference learning, anomaly detection
- [ ] **Expression:** LED animations, motor movements, tone variation
- [ ] **Multi-User Support:** Per-user profiles, personalized interactions

---

## üìñ Documentation

Comprehensive guides organized by audience and use case. **Start here:**

### For First-Time Users
1. **[Quick Start](#quick-start)** (below) ‚Äî Run the system in 5 minutes
2. **[010_PROJECT_GOALS_AND_SETUP.md](010_PROJECT_GOALS_AND_SETUP.md)** ‚Äî Jetson setup, ROS 2 installation, workspace structure
3. **[001_ARCHITECTURE_OVERVIEW.md](001_ARCHITECTURE_OVERVIEW.md)** ‚Äî System design, software stack, data flow

### For Daily Operators
- **[QUICK_START.md](QUICK_START.md)** ‚Äî Quick reference for common tasks
- **[AUDIO_QUICK_REFERENCE.md](AUDIO_QUICK_REFERENCE.md)** ‚Äî Audio system quick reference

### For Phase 1 (Current) Developers
- **[000_INTERNAL_AGENT_NOTES.md](000_INTERNAL_AGENT_NOTES.md)** ‚Äî ARM architecture quirks, DepthAI setup, common issues and solutions
- **[002_HOW_TO_INSTRUCT_CLAUDE.md](002_HOW_TO_INSTRUCT_CLAUDE.md)** ‚Äî Guidelines for working with AI agents on this project

### Technical Depth (Phase 1 Subsystems)
| Component | Document | Purpose |
|-----------|----------|---------|
| **Foundations** | [000_INTERNAL_AGENT_NOTES.md](000_INTERNAL_AGENT_NOTES.md) | Critical git rules, environment setup, hardware constants |
| **Architecture** | [001_ARCHITECTURE_OVERVIEW.md](001_ARCHITECTURE_OVERVIEW.md) | System design, software stack, ROS 2 topics |
| **System Setup** | [010_PROJECT_GOALS_AND_SETUP.md](010_PROJECT_GOALS_AND_SETUP.md) | Jetson flashing, ROS 2 installation, workspace setup |
| **Camera Integration** | [041_CAMERA_SETUP_DOCUMENTATION.md](041_CAMERA_SETUP_DOCUMENTATION.md) | OAK-D Lite + DepthAI SDK, ROS 2 camera_node |
| **Perception & Face Recognition** | [040_FACE_RECOGNITION_COMPLETE.md](040_FACE_RECOGNITION_COMPLETE.md) | Perception pipeline, brightness metrics, LBPH training, real-time recognition |
| **Audio Hardware** | [050_AUDIO_SETUP_AND_CONFIGURATION.md](050_AUDIO_SETUP_AND_CONFIGURATION.md) | ALSA configuration, ffplay setup, PAM8403 amplifier |
| **üëâ Person Recognition (MAIN REF)** | **[070_PERSON_RECOGNITION_STATUS.md](070_PERSON_RECOGNITION_STATUS.md)** | **Complete state machine, audio alerts, LED feedback, testing, troubleshooting** |
| **ROS 2 Audio Integration** | [060_AUDIO_NOTIFICATIONS_ROS2_INTEGRATION.md](060_AUDIO_NOTIFICATIONS_ROS2_INTEGRATION.md) | Audio ROS 2 node implementation, systemd service setup |
| **Power Button Control** | [080_POWER_BUTTON_FINAL_DOCUMENTATION.md](080_POWER_BUTTON_FINAL_DOCUMENTATION.md) | Shutdown (Pin 32) + boot/wake (J42) control, tested ‚úÖ |
| **Backup & Restore** | [004_BACKUP_AND_RESTORE.md](004_BACKUP_AND_RESTORE.md) | Full-system backup for reproducible deployments |

---

## ‚úÖ Phase 1 Status

**Completion:** ~85% (core systems operational, documentation polishing)

![R2D2 Empty Body](docs/photos/empty_body.jpg)

### What's Working
- ‚úÖ **Jetson Setup:** JetPack 6.x, ROS 2 Humble, clean workspace structure
- ‚úÖ **Camera:** OAK-D Lite streaming 30 FPS RGB (1920√ó1080) at `/oak/rgb/image_raw`
- ‚úÖ **Perception:** Brightness metrics + Haar Cascade face detection (13 Hz) + LBPH recognition (6.5 Hz)
- ‚úÖ **Node Architecture:** 4 ROS 2 packages with parameter-driven configuration
- ‚úÖ **Performance:** ~10-15% CPU usage (perception pipeline), excellent thermal stability
- ‚úÖ **Documentation:** 7+ technical guides + architecture diagrams + integration templates

### Remaining Phase 1 Tasks
- ‚è≥ **README Improvements** (this file ‚Äî making it more accessible)
- ‚è≥ **PROJECT_GOALS.md** (complete roadmap + 4-phase vision)
- ‚è≥ Git commit of final documentation

### Phase 1 Exit Criteria
When all above are done:
- ‚úÖ New developers can start camera stream in 5 minutes
- ‚úÖ Architecture is clear (blocks, data flow, integration points)
- ‚úÖ Existing documentation is audited and organized
- ‚úÖ Path to Phase 2 (Speech) is documented
- **‚Üí Ready to start Phase 2**

---

## üìä Project Status & Roadmap

### Overall Progress: 30% Complete (16 Core Objectives)

| Domain | Objective | Phase | Status | Notes |
|--------|-----------|-------|--------|-------|
| **Conversation** | Natural voice interaction | 2 | ‚è≥ Next | STT + LLM + TTS pipeline |
| **Conversation** | Multi-turn dialogue | 2 | ‚è≥ Next | Requires Phase 2 |
| **Conversation** | Local AI processing | 2 | ‚è≥ Next | Ollama + Llama 2 7B |
| **Perception** | Face recognition | 1 | ‚úÖ Done | LBPH trained, 85-92% accuracy |
| **Perception** | Person tracking | 3 | ‚è≥ Future | Requires navigation Phase |
| **Perception** | Object detection | 3 | ‚è≥ Future | Post-Phase 2 |
| **Perception** | Room understanding | 3 | ‚è≥ Future | SLAM + spatial mapping |
| **Perception** | Situation awareness | 1 | ‚úÖ Partial | Brightness + face count available |
| **Navigation** | Indoor navigation | 3 | ‚è≥ Future | Requires motor + Nav2 integration |
| **Navigation** | Mapping & localization | 3 | ‚è≥ Future | SLAM with OAK-D depth |
| **Navigation** | Obstacle avoidance | 3 | ‚è≥ Future | Real-time replanning |
| **Navigation** | Safe movement | 3 | ‚è≥ Future | Speed limiting + collision detection |
| **Expression** | Expressive audio | 2 | ‚è≥ Next | Beep synthesis + sound files |
| **Expression** | Social responsiveness | 2 | ‚è≥ Next | Multi-modal coordination |
| **Expression** | Directional awareness | 1 | ‚úÖ Partial | Camera orientation possible |
| **Commands** | Verbal command interface | 2 | ‚è≥ Next | After STT + LLM |

### Phase Breakdown

```
Phase 1: Core System Bringup (30% total project)
‚îú‚îÄ ‚úÖ Jetson + ROS 2 setup
‚îú‚îÄ ‚úÖ Camera integration (30 FPS RGB streaming)
‚îú‚îÄ ‚úÖ Perception pipeline (brightness + face detection/recognition)
‚îú‚îÄ ‚úÖ Professional documentation
‚îî‚îÄ üìç Current: ~85% complete

Phase 2: Speech & Language (35% total project)
‚îú‚îÄ ‚úÖ Microphone integration (HyperX QuadCast S USB - working perfectly)
‚îú‚îÄ ‚úÖ Speech-to-Text (Faster-Whisper large-v2 - 100% accuracy)
‚îú‚îÄ ‚úÖ Local LLM (Grok-3 API - intelligent contextual responses)
‚îú‚îÄ ‚úÖ Text-to-Speech (Piper TTS German - natural synthesis)
‚îú‚îÄ ‚è≥ GPU acceleration for STT (PyTorch CUDA setup pending)
‚îú‚îÄ ‚è≥ Speaker output (PAM8403 - hardware issue, needs debugging)
‚îî‚îÄ üìç Current: ~90% complete (software), speaker hardware blocked

Phase 3: Navigation & Movement (20% total project)
‚îú‚îÄ Motor control (PWM drivers, encoders)
‚îú‚îÄ SLAM mapping (OAK-D depth + visual odometry)
‚îú‚îÄ Nav2 path planning
‚îú‚îÄ Obstacle avoidance
‚îî‚îÄ üéØ Target: 8-10 weeks after Phase 2

Phase 4: Memory & Personality (15% total project)
‚îú‚îÄ Conversation memory (SQLite, context management)
‚îú‚îÄ Learning & adaptation (reinforcement from feedback)
‚îú‚îÄ LED animations + motor expressions
‚îú‚îÄ Multi-user support
‚îî‚îÄ üéØ Target: 10+ weeks after Phase 3
```

---

## üîÑ Recommended Next Steps (Your Priorities)

Based on your goals to implement **mic & speaker integration ‚Üí STT/LLM/TTS**, here's the optimal order:

### Step 1: GPU Acceleration for STT (Week 1)
**Goal:** Enable PyTorch GPU support for 4-6x STT speed improvement

**Tasks:**
1. ‚è≥ Reinstall PyTorch with CUDA 12.4 support (from CPU-only build)
2. ‚è≥ Update STT config to use GPU device
3. ‚è≥ Optimize STT model (large-v2 ‚Üí base)
4. ‚è≥ Verify GPU acceleration working

**Status:** See `999_NEXT_TASKS.md` for detailed implementation guide

**Deliverable:**
- STT processing time: 3-5 sec ‚Üí 0.5-0.8 sec (6x improvement)
- Total conversation latency: 15-20 sec ‚Üí 5-7 sec
- Document in phase-specific guides

---

### Step 2: Speech-to-Text (Week 2-3)
**Goal:** Convert spoken audio to text in real-time

**Tasks:**
1. ‚è≥ Evaluate STT options:
   - Whisper (OpenAI, CPU-friendly, good accuracy)
   - Vosk (lightweight, offline, less accurate)
   - Google Speech Recognition (requires cloud - not preferred)
2. ‚è≥ Install selected STT engine in depthai_env
3. ‚è≥ Create `r2d2_speech_to_text` node
4. ‚è≥ Subscribe to `/r2d2/audio/raw`, output to `/r2d2/speech/text`
5. ‚è≥ Add wake-word detection ("Hey R2D2")
6. ‚è≥ Benchmark: latency, accuracy, CPU usage

**Deliverable:**
- Working STT node
- Wake-word detection triggering transcription
- <1 second latency from speech end to text output
- CPU usage metrics documented

---

### Step 3: Local Language Model (Week 3-5)
**Goal:** Generate contextual responses from text input

**Tasks:**
1. ‚è≥ Install Ollama (LLM inference framework)
2. ‚è≥ Download Llama 2 7B model (fits in Jetson AGX memory)
3. ‚è≥ Test inference speed (target: <2 sec for response)
4. ‚è≥ Create `r2d2_language_model` node
5. ‚è≥ Subscribe to `/r2d2/speech/text`
6. ‚è≥ Add context awareness (recent face recognition, brightness, etc.)
7. ‚è≥ Output to `/r2d2/ai/response`

**Deliverable:**
- LLM node generating contextual responses
- Integration with perception system (greet by name, reference environment)
- <2 second response time
- CPU/memory profiling

---

### Step 4: Text-to-Speech (Week 5-6)
**Goal:** Synthesize spoken responses with natural voice

**Tasks:**
1. ‚è≥ Evaluate TTS options:
   - pyttsx3 (lightweight, offline, multiple voices)
   - glow-tts (higher quality, more CPU)
   - espeak (minimal resources)
2. ‚è≥ Install and test selected TTS
3. ‚è≥ Create `r2d2_text_to_speech` node
4. ‚è≥ Subscribe to `/r2d2/ai/response`
5. ‚è≥ Generate audio frames to `/r2d2/audio/output`
6. ‚è≥ Benchmark: latency, naturalness, CPU usage

**Deliverable:**
- Full pipeline: speech ‚Üí text ‚Üí understanding ‚Üí response ‚Üí speech
- <3 second total latency (audio in to audio out)
- Usable voice quality

---

### Step 5: Full Integration & Testing (Week 6-8)
**Goal:** End-to-end conversational AI system

**Tasks:**
1. ‚è≥ Connect all nodes with proper message flow
2. ‚è≥ Add error handling (no speech, no response, etc.)
3. ‚è≥ Implement conversation memory (last 5 exchanges)
4. ‚è≥ Test with real use cases:
   - "Hello R2D2, what's your name?" 
   - "What do you see?" (use perception data)
   - "Who am I?" (face recognition)
   - Multi-turn dialogue
5. ‚è≥ Performance optimization:
   - Profile CPU/GPU/memory
   - Optimize model inference
   - Document resource usage
6. ‚è≥ Update documentation and examples

**Deliverable:**
- Complete Phase 2 system ready for Phase 3
- All components documented and tested
- Performance baseline: <3 sec end-to-end latency
- Ready to add navigation features

---

## üìà Expected Challenges & Solutions

| Challenge | Phase | Solution |
|-----------|-------|----------|
| STT runs on CPU (slow) | 2.1 | Install PyTorch with CUDA support, use smaller model (Task #0) |
| Speaker amplifier not working | 2.2 | Hardware debugging needed (power/wiring issue), fallback to file output |
| Latency still high | 2.3 | Enable GPU acceleration, optimize model inference |
| Integration with perception | 2.4 | Wire speech to `/r2d2/perception/person_id` for context-aware responses |
| Conversation context memory | 2.5 | Add conversation history to LLM prompts, implement memory system |

---

---

## üîß Hardware

The iconic R2-D2 is being rebuilt with modern robotics hardware and AI compute.

### Current Hardware Stack
| Component | Model | Purpose | Status |
|-----------|-------|---------|--------|
| **Chassis** | DeAgostini R2-D2 1:2 Kit | Main body (48 cm tall) | ‚úÖ Complete |
| **Compute** | NVIDIA Jetson AGX Orin 64GB | AI brain (12-core ARM, 504-GPU cores, 100W TDP) | ‚úÖ Mounted & Running |
| **Camera** | Luxonis OAK-D Lite Auto Focus | Vision (1920√ó1080 @ 30 FPS, depth + IMU) | ‚úÖ Integrated |
| **Audio Input** | HyperX QuadCast S USB | Voice capture (44.1kHz stereo) | ‚úÖ Working |
| **Audio Output** | PAM8403 Amplifier + Speaker | Voice synthesis playback | ‚ùå Hardware Issue |
| **Drive** | DeAgostini DC Motors (2√ó) | Leg & dome motors with encoders | ‚è≥ Not yet integrated |
| **Motor Control** | Pololu MC33926 (2√ó) | H-bridge drivers for DC motors | ‚úÖ Assembled |
| **Power** | 4S LiPo 5000 mAh (14.8V) | Main battery system | ‚úÖ Charged & Ready |
| **Power Dist** | Custom DC-DC (14V‚Üí12V/5V) | Jetson + peripherals + motors | ‚è≥ Partial (Jetson only) |
| **Internal** | WS2812B RGB LEDs | Status & personality expression | ‚è≥ Coming in Phase 4 |

### Inside the Bot
![R2D2 Body With Jetson](docs/photos/body%20with%20jetson.jpg)  
The internal structure houses the Jetson AGX Orin, OAK-D camera, power distribution, and future motor drivers. Careful cable management ensures room for Phase 2-4 additions.

### Bill of Materials (Full Project)
See the **Hardware** section below and [003_JETSON_FLASHING_AND_DISPLAY_SETUP.md](003_JETSON_FLASHING_AND_DISPLAY_SETUP.md) for component details (~$3,600 including chassis).



## Repository Structure

The repository reflects the active development environment on the Jetson AGX Orin.  
Generated ROS 2 build artifacts are excluded via `.gitignore`, resulting in a clean and minimal source tree.

```text
.
‚îú‚îÄ ros2_ws/
‚îÇ  ‚îú‚îÄ src/
‚îÇ  ‚îÇ  ‚îú‚îÄ r2d2_hello/      # First functional nodes (beep + heartbeat)
‚îÇ  ‚îÇ  ‚îî‚îÄ r2d2_bringup/    # Bringup launch to start the robot system
‚îÇ  ‚îú‚îÄ build/              # (generated, ignored)
‚îÇ  ‚îú‚îÄ install/            # (generated, ignored)
‚îÇ  ‚îî‚îÄ log/                # (generated, ignored)
‚îú‚îÄ docs/
‚îÇ  ‚îî‚îÄ photos/             # Build documentation images
‚îú‚îÄ tests/                 # GPU/Audio/Camera ‚Äútouch-the-ground‚Äù tests (planned)
‚îî‚îÄ scripts/               # Utility scripts
```



## üöÄ Quick Start

### 1. Clone & Setup (5 min)

```bash
# Clone repository
git clone git@github.com:severinleuenberger/R2D2-as-real-AI-companion.git
cd R2D2-as-real-AI-companion

# Set up environment (CRITICAL: order matters!)
source ~/depthai_env/bin/activate
export OPENBLAS_CORETYPE=ARMV8
source ~/.bashrc
cd ros2_ws
source install/setup.bash
```

### 2. Launch Camera & Perception (1 command)

```bash
ros2 launch r2d2_bringup r2d2_camera_perception.launch.py
```

**Expected Output (first 5 seconds):**
```
[INFO] [camera_node]: OAK-D camera initialized
[INFO] [image_listener]: ImageListener node initialized
[INFO] [image_listener]: Haar Cascade loaded successfully
[INFO] All nodes started successfully
```

### 3. Verify System (In another terminal, same env setup as above)

```bash
# Check camera frame rate (should be ~30 Hz)
ros2 topic hz /oak/rgb/image_raw

# Check perception output (should be ~13 Hz)
ros2 topic hz /r2d2/perception/brightness

# See brightness values (should be ~130-140 in normal light)
watch -n 0.5 'ros2 topic echo /r2d2/perception/brightness -n 1'

# See face detection (0 = no one, 1+ = detected)
ros2 topic echo /r2d2/perception/face_count
```

### 4. Troubleshooting

If things don't work:
1. Check **[Troubleshooting](#troubleshooting)** section below or [001_ARCHITECTURE_OVERVIEW.md](001_ARCHITECTURE_OVERVIEW.md)
2. Verify environment setup: `echo $OPENBLAS_CORETYPE` should print `ARMV8`
3. Check camera: `lsusb | grep Movidius`

**Time to working system:** ~7 seconds from launch command

---

## üìÅ Repository Structure

```
r2d2/
‚îú‚îÄ‚îÄ README.md                              # This file
‚îú‚îÄ‚îÄ 000_INTERNAL_AGENT_NOTES.md             # ARM quirks, DepthAI setup, performance baselines
‚îú‚îÄ‚îÄ 001_ARCHITECTURE_OVERVIEW.md            # System design, data flow, integration patterns
‚îú‚îÄ‚îÄ 002_HOW_TO_INSTRUCT_CLAUDE.md           # How to ask Claude for R2D2 task help
‚îú‚îÄ‚îÄ 003_JETSON_FLASHING_AND_DISPLAY_SETUP.md # Hardware setup procedures
‚îú‚îÄ‚îÄ 004_BACKUP_AND_RESTORE.md              # Backup/restore procedures
‚îú‚îÄ‚îÄ 010_PROJECT_GOALS_AND_SETUP.md         # 4-phase roadmap, success metrics
‚îú‚îÄ‚îÄ 041_CAMERA_SETUP_DOCUMENTATION.md      # OAK-D camera + DepthAI SDK
‚îú‚îÄ‚îÄ 040_FACE_RECOGNITION_COMPLETE.md       # Perception pipeline + Face recognition system
‚îú‚îÄ‚îÄ 050_AUDIO_SETUP_AND_CONFIGURATION.md   # Audio system setup
‚îú‚îÄ‚îÄ 060_AUDIO_NOTIFICATIONS_ROS2_INTEGRATION.md # Audio integration
‚îú‚îÄ‚îÄ OPERATIONS_CHECKLIST.md                # Daily startup, monitoring, troubleshooting
‚îú‚îÄ‚îÄ INTEGRATION_GUIDE.md                   # How to add Phase 2-4 features (template + examples)
‚îú‚îÄ‚îÄ COMPUTE_COST_ANALYSIS.md               # Performance profiles
‚îÇ
‚îú‚îÄ‚îÄ ros2_ws/                               # ROS 2 Humble workspace
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ r2d2_camera/                  # Camera node
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ r2d2_perception/              # Perception node
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ r2d2_hello/                   # Heartbeat + beep
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ r2d2_bringup/                 # Launch files
‚îÇ   ‚îú‚îÄ‚îÄ build/ install/ log/              # (generated, gitignored)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ face_recognition/models/          # Trained LBPH models
‚îÇ
‚îú‚îÄ‚îÄ tests/                                 # Component test scripts
‚îú‚îÄ‚îÄ docs/photos/                           # Build progress photos
‚îî‚îÄ‚îÄ scripts/                               # Utility scripts
```

---

## üîó Bill of Materials (BOM)

See [010_PROJECT_GOALS_AND_SETUP.md](010_PROJECT_GOALS_AND_SETUP.md) ‚Üí "Hardware & Resource Requirements" section for detailed breakdown.

**Quick Summary:**
- **Total cost (compute + sensors):** ~$2,200
- **With DeAgostini kit:** ~$3,600
- **Key sources:** NVIDIA, Luxonis, HobbyKing, Pololu, Seeed Studio

---

## ü§ù Contributing & Community

**GitHub:** [severinleuenberger/R2D2-as-real-AI-companion](https://github.com/severinleuenberger/R2D2-as-real-AI-companion)

**Discussion:**
- [NVIDIA Developer Forums](https://forums.developer.nvidia.com/) (R2D2 thread)
- [GitHub Discussions](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/discussions)

**How to Contribute:**
- **Found a bug?** ‚Üí [Create an Issue](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/issues)
- **Have an improvement?** ‚Üí [Submit a Pull Request](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/pulls)
- **Want to discuss?** ‚Üí [Start a Discussion](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/discussions)
- **Building your own R2D2?** ‚Üí Document it and share!

**License:** [MIT](LICENSE) - Free to use, modify, and distribute

**Author:** [@s_leuenberger](https://x.com/s_leuenberger) | Switzerland

---

## üéØ What's Next?

1. **This week:** Complete Phase 1 documentation
2. **Next week:** Begin Phase 2 (speech-to-text) prototype
3. **Next month:** Full speech + LLM pipeline
4. **Q2 2026:** Navigation and autonomous movement
5. **Q3 2026+:** Memory, personality, and deployment

See [010_PROJECT_GOALS_AND_SETUP.md](010_PROJECT_GOALS_AND_SETUP.md) for the complete Phase 1 roadmap and Phase 2-4 timeline.

---

**Happy building! Questions?** Check the [docs](README.md#-documentation) or [open an issue](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/issues).

