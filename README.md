# R2D2 as a Real AI Companion

> Transform the iconic 1:2 DeAgostini R2-D2 into a fully autonomous AI companion robot with vision, voice, navigation, and personality‚Äîpowered by NVIDIA Jetson AGX Orin 64GB and ROS 2 Humble.

![R2D2 Full Robot](docs/photos/20251107_105518.jpg)

**Current Status:** Phase 1 Core System ‚Äî ~85% complete  
**Next Phase:** Speech & Language (STT + LLM + TTS)  
**Project Timeline:** 4 phases, 280-300 hours total (7 weeks full-time, 3-4 months part-time)

---

## üéØ Core Objectives

The R2D2 project has 16 fundamental goals that define what the robot must accomplish:

### Conversation & Language
- **Natural Voice Interaction:** Listen to spoken input, convert to text, understand meaning, generate contextual responses, and speak back with natural-sounding voice
- **Multi-Turn Conversation:** Support continuous dialogue without manual resets or repeated wake-word calls
- **Local AI Processing:** All speech-to-text, language model, and text-to-speech run locally without cloud services (privacy + offline capability)

### Perception & Recognition
- **Face Recognition:** Detect and identify people, especially primary user (Severin)
- **Person Tracking:** Track identified individuals and optionally follow them autonomously
- **Object Detection:** Recognize obstacles, furniture, and items using camera and depth sensors
- **Environment Understanding:** Recognize rooms and understand spatial context (kitchen, bedroom, office, etc.)
- **Situation Awareness:** Detect environmental events (person entering room, name being called, obstacles appearing)

### Autonomous Navigation
- **Indoor Navigation:** Move autonomously through multi-room environments without human guidance
- **Mapping & Localization:** Build and maintain maps, know current position, recognize revisited areas
- **Obstacle Avoidance:** Detect and avoid collisions with furniture, people, pets, and other obstacles
- **Safe Movement:** Maintain appropriate speeds and stop distances to prevent accidents
- **Location-Based Commands:** Respond to verbal commands like "go to kitchen" or "come here"

### Expression & Interaction
- **Expressive Audio:** Generate R2-D2-like beep sounds and emotional vocalizations
- **Social Responsiveness:** React to environmental events and social cues in real-time
- **Directional Awareness:** Orient camera and movement toward person speaking or object of interest
- **Multi-Modal Interaction:** Simultaneously perceive, navigate, and converse while maintaining social engagement
- **Heartbeat Signal:** Provide continuous "alive" indicator that system is operational
- **Sound Playback:** Play audio files including R2-D2 sound effects and music

### Verbal Command Interface
- **Direct Commands:** Accept and execute commands like "follow me," "look at this," "go to the living room"
- **Natural Language:** Understand conversational requests, not just rigid command syntax
- **Context Awareness:** Remember conversation history and current physical context

---

---

## üöÄ Features (Current & Planned)

### Phase 1: Core System (Current) ‚úÖ
- [x] **Real-time Perception:** 30 FPS RGB camera stream, brightness metrics, face detection (90% accuracy)
- [x] **Face Recognition:** LBPH-based person identification (trained for multiple users)
- [x] **Hardware Integration:** OAK-D Lite depth camera, Jetson AGX Orin compute, ROS 2 infrastructure
- [x] **Professional Codebase:** Clean workspace structure, modular packages, parameter-driven configuration
- [x] **Comprehensive Docs:** Setup guides, integration patterns, operations checklist, architecture diagrams

### Phase 2: Speech & AI (Next) ‚è≥
- [ ] **Speech-to-Text:** Real-time audio input with wake-word detection ("Hey R2D2")
- [ ] **Language Model:** Local LLM inference (Llama 2 7B) for conversational responses
- [ ] **Text-to-Speech:** Real-time voice synthesis and playback
- [ ] **Context Awareness:** Vision-informed responses (greet by name, reference visual context)

### Phase 3: Navigation (Future) ‚è≥
- [ ] **SLAM Mapping:** Autonomous room mapping and localization
- [ ] **Autonomous Movement:** Differential drive control for 2-wheel locomotion
- [ ] **Obstacle Avoidance:** Real-time collision detection and path replanning
- [ ] **Room Navigation:** Go to named rooms, return to base, explore autonomously

### Phase 4: Memory & Personality (Future) ‚è≥
- [ ] **Conversation Memory:** Persistent history, context-aware responses
- [ ] **Learning & Adaptation:** Preference learning, anomaly detection
- [ ] **Expression:** LED animations, motor movements, tone variation
- [ ] **Multi-User Support:** Per-user profiles, personalized interactions

---

## üìñ Documentation

Comprehensive guides organized by audience and use case. **Start here:**

### For First-Time Users
1. **[Quick Start](#quick-start)** (below) ‚Äî Run the system in 5 minutes
2. **[PROJECT_GOALS.md](PROJECT_GOALS.md)** ‚Äî Understand the 4-phase roadmap and success metrics
3. **[ARCHITECTURE_OVERVIEW.md](ARCHITECTURE_OVERVIEW.md)** ‚Äî See how components fit together

### For Daily Operators
- **[OPERATIONS_CHECKLIST.md](OPERATIONS_CHECKLIST.md)** ‚Äî Startup, monitoring, troubleshooting, recovery procedures
- **[COMPUTE_COST_ANALYSIS.md](COMPUTE_COST_ANALYSIS.md)** ‚Äî CPU/memory usage, performance baselines, optimization tips

### For Phase 2-4 Developers
- **[INTEGRATION_GUIDE.md](INTEGRATION_GUIDE.md)** ‚Äî Step-by-step guide to add speech, navigation, memory systems
- **[00_INTERNAL_AGENT_NOTES.md](00_INTERNAL_AGENT_NOTES.md)** ‚Äî ARM architecture quirks, DepthAI setup, common issues and solutions

### Technical Depth (Phase 1 Subsystems)
| Component | Document | Purpose |
|-----------|----------|---------|
| **System Setup** | [01_BASIC_SETUP_AND_FINDINGS.md](01_R2D2_BASIC_SETUP_AND_FINDINGS.md) | Jetson flashing, ROS 2 installation, workspace setup |
| **Camera Integration** | [02_CAMERA_SETUP_DOCUMENTATION.md](02_CAMERA_SETUP_DOCUMENTATION.md) | OAK-D Lite + DepthAI SDK, ROS 2 camera_node |
| **Image Processing** | [03_PERCEPTION_SETUP_DOCUMENTATION.md](03_PERCEPTION_SETUP_DOCUMENTATION.md) | Brightness metrics, Haar Cascade face detection, pipeline |
| **Face Recognition** | [05_FACE_RECOGNITION_INTEGRATION.md](05_FACE_RECOGNITION_INTEGRATION.md) & [06_FACE_RECOGNITION_TRAINING_AND_STATUS.md](06_FACE_RECOGNITION_TRAINING_AND_STATUS.md) | LBPH training, real-time recognition, model management |
| **Backup & Restore** | [07_BACKUP_AND_RESTORE_SETUP.md](07_BACKUP_AND_RESTORE_SETUP.md) | Full-system backup for reproducible deployments |

---

## ‚úÖ Phase 1 Status

**Completion:** ~85% (core systems operational, documentation polishing)

![R2D2 Empty Body](docs/photos/empty_body.jpg)

### What's Working
- ‚úÖ **Jetson Setup:** JetPack 6.x, ROS 2 Humble, clean workspace structure
- ‚úÖ **Camera:** OAK-D Lite streaming 30 FPS RGB (1920√ó1080) at `/oak/rgb/image_raw`
- ‚úÖ **Perception:** Brightness metrics + Haar Cascade face detection (13 Hz) + LBPH recognition (6.5 Hz)
- ‚úÖ **Node Architecture:** 4 ROS 2 packages with parameter-driven configuration
- ‚úÖ **Performance:** ~10-15% CPU usage (perception pipeline), excellent thermal stability
- ‚úÖ **Documentation:** 7+ technical guides + architecture diagrams + integration templates

### Remaining Phase 1 Tasks
- ‚è≥ **README Improvements** (this file ‚Äî making it more accessible)
- ‚è≥ **PROJECT_GOALS.md** (complete roadmap + 4-phase vision)
- ‚è≥ Git commit of final documentation

### Phase 1 Exit Criteria
When all above are done:
- ‚úÖ New developers can start camera stream in 5 minutes
- ‚úÖ Architecture is clear (blocks, data flow, integration points)
- ‚úÖ Existing documentation is audited and organized
- ‚úÖ Path to Phase 2 (Speech) is documented
- **‚Üí Ready to start Phase 2**



---

## üîß Hardware

The iconic R2-D2 is being rebuilt with modern robotics hardware and AI compute.

### Current Hardware Stack
| Component | Model | Purpose | Status |
|-----------|-------|---------|--------|
| **Chassis** | DeAgostini R2-D2 1:2 Kit | Main body (48 cm tall) | ‚úÖ Complete |
| **Compute** | NVIDIA Jetson AGX Orin 64GB | AI brain (12-core ARM, 504-GPU cores, 100W TDP) | ‚úÖ Mounted & Running |
| **Camera** | Luxonis OAK-D Lite Auto Focus | Vision (1920√ó1080 @ 30 FPS, depth + IMU) | ‚úÖ Integrated |
| **Audio Input** | ReSpeaker 2-Mic HAT | Voice capture (for Phase 2 STT) | ‚è≥ Ordered |
| **Drive** | DeAgostini DC Motors (2√ó) | Leg & dome motors with encoders | ‚è≥ Not yet integrated |
| **Motor Control** | Pololu MC33926 (2√ó) | H-bridge drivers for DC motors | ‚úÖ Assembled |
| **Power** | 4S LiPo 5000 mAh (14.8V) | Main battery system | ‚úÖ Charged & Ready |
| **Power Dist** | Custom DC-DC (14V‚Üí12V/5V) | Jetson + ReSpeaker + motors | ‚è≥ Not yet integrated |
| **Internal** | WS2812B RGB LEDs | Status & personality expression | ‚è≥ Coming in Phase 4 |

### Inside the Bot
![R2D2 Body With Jetson](docs/photos/body%20with%20jetson.jpg)  
The internal structure houses the Jetson AGX Orin, OAK-D camera, power distribution, and future motor drivers. Careful cable management ensures room for Phase 2-4 additions.

### Bill of Materials (Full Project)
See [BOM_HARDWARE.md](BOM_HARDWARE.md) for detailed part numbers, sourcing links, and cost breakdown (~$3,600 including chassis).



## Repository Structure

The repository reflects the active development environment on the Jetson AGX Orin.  
Generated ROS 2 build artifacts are excluded via `.gitignore`, resulting in a clean and minimal source tree.

```text
.
‚îú‚îÄ ros2_ws/
‚îÇ  ‚îú‚îÄ src/
‚îÇ  ‚îÇ  ‚îú‚îÄ r2d2_hello/      # First functional nodes (beep + heartbeat)
‚îÇ  ‚îÇ  ‚îî‚îÄ r2d2_bringup/    # Bringup launch to start the robot system
‚îÇ  ‚îú‚îÄ build/              # (generated, ignored)
‚îÇ  ‚îú‚îÄ install/            # (generated, ignored)
‚îÇ  ‚îî‚îÄ log/                # (generated, ignored)
‚îú‚îÄ docs/
‚îÇ  ‚îî‚îÄ photos/             # Build documentation images
‚îú‚îÄ tests/                 # GPU/Audio/Camera ‚Äútouch-the-ground‚Äù tests (planned)
‚îî‚îÄ scripts/               # Utility scripts
```



## üöÄ Quick Start

### 1. Clone & Setup (5 min)

```bash
# Clone repository
git clone git@github.com:severinleuenberger/R2D2-as-real-AI-companion.git
cd R2D2-as-real-AI-companion

# Set up environment (CRITICAL: order matters!)
source ~/depthai_env/bin/activate
export OPENBLAS_CORETYPE=ARMV8
source ~/.bashrc
cd ros2_ws
source install/setup.bash
```

### 2. Launch Camera & Perception (1 command)

```bash
ros2 launch r2d2_bringup r2d2_camera_perception.launch.py
```

**Expected Output (first 5 seconds):**
```
[INFO] [camera_node]: OAK-D camera initialized
[INFO] [image_listener]: ImageListener node initialized
[INFO] [image_listener]: Haar Cascade loaded successfully
[INFO] All nodes started successfully
```

### 3. Verify System (In another terminal, same env setup as above)

```bash
# Check camera frame rate (should be ~30 Hz)
ros2 topic hz /oak/rgb/image_raw

# Check perception output (should be ~13 Hz)
ros2 topic hz /r2d2/perception/brightness

# See brightness values (should be ~130-140 in normal light)
watch -n 0.5 'ros2 topic echo /r2d2/perception/brightness -n 1'

# See face detection (0 = no one, 1+ = detected)
ros2 topic echo /r2d2/perception/face_count
```

### 4. Troubleshooting

If things don't work:
1. Check [OPERATIONS_CHECKLIST.md](OPERATIONS_CHECKLIST.md) ‚Üí Section 5 (Troubleshooting)
2. Verify environment setup: `echo $OPENBLAS_CORETYPE` should print `ARMV8`
3. Check camera: `lsusb | grep Movidius`

**Time to working system:** ~7 seconds from launch command

---

## üìÅ Repository Structure

```
r2d2/
‚îú‚îÄ‚îÄ README.md                              # This file
‚îú‚îÄ‚îÄ PROJECT_GOALS.md                       # 4-phase roadmap, success metrics, FAQs
‚îú‚îÄ‚îÄ ARCHITECTURE_OVERVIEW.md               # System design, data flow, integration patterns
‚îú‚îÄ‚îÄ OPERATIONS_CHECKLIST.md                # Daily startup, monitoring, troubleshooting
‚îú‚îÄ‚îÄ INTEGRATION_GUIDE.md                   # How to add Phase 2-4 features (template + examples)
‚îú‚îÄ‚îÄ 00_INTERNAL_AGENT_NOTES.md             # ARM quirks, DepthAI setup, performance baselines
‚îú‚îÄ‚îÄ 01_R2D2_BASIC_SETUP_AND_FINDINGS.md   # Jetson setup, ROS 2 installation
‚îú‚îÄ‚îÄ 02_CAMERA_SETUP_DOCUMENTATION.md      # OAK-D camera + DepthAI SDK
‚îú‚îÄ‚îÄ 03_PERCEPTION_SETUP_DOCUMENTATION.md  # Image processing pipeline
‚îú‚îÄ‚îÄ 05_FACE_RECOGNITION_INTEGRATION.md    # Face recognition system
‚îú‚îÄ‚îÄ 06_FACE_RECOGNITION_TRAINING_AND_STATUS.md # Training + status
‚îú‚îÄ‚îÄ 07_BACKUP_AND_RESTORE_SETUP.md        # Backup/restore procedures
‚îú‚îÄ‚îÄ COMPUTE_COST_ANALYSIS.md               # Performance profiles
‚îÇ
‚îú‚îÄ‚îÄ ros2_ws/                               # ROS 2 Humble workspace
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ r2d2_camera/                  # Camera node
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ r2d2_perception/              # Perception node
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ r2d2_hello/                   # Heartbeat + beep
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ r2d2_bringup/                 # Launch files
‚îÇ   ‚îú‚îÄ‚îÄ build/ install/ log/              # (generated, gitignored)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ face_recognition/models/          # Trained LBPH models
‚îÇ
‚îú‚îÄ‚îÄ tests/                                 # Component test scripts
‚îú‚îÄ‚îÄ docs/photos/                           # Build progress photos
‚îî‚îÄ‚îÄ scripts/                               # Utility scripts
```

---

## üîó Bill of Materials (BOM)

See [PROJECT_GOALS.md](PROJECT_GOALS.md) ‚Üí "Resource Requirements" section for detailed hardware breakdown.

**Quick Summary:**
- **Total cost (compute + sensors):** ~$2,200
- **With DeAgostini kit:** ~$3,600
- **Key sources:** NVIDIA, Luxonis, HobbyKing, Pololu, Seeed Studio

---

## ü§ù Contributing & Community

**GitHub:** [severinleuenberger/R2D2-as-real-AI-companion](https://github.com/severinleuenberger/R2D2-as-real-AI-companion)

**Discussion:**
- [NVIDIA Developer Forums](https://forums.developer.nvidia.com/) (R2D2 thread)
- [GitHub Discussions](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/discussions)

**How to Contribute:**
- **Found a bug?** ‚Üí [Create an Issue](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/issues)
- **Have an improvement?** ‚Üí [Submit a Pull Request](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/pulls)
- **Want to discuss?** ‚Üí [Start a Discussion](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/discussions)
- **Building your own R2D2?** ‚Üí Document it and share!

**License:** [MIT](LICENSE) - Free to use, modify, and distribute

**Author:** [@s_leuenberger](https://x.com/s_leuenberger) | Switzerland

---

## üéØ What's Next?

1. **This week:** Complete Phase 1 documentation
2. **Next week:** Begin Phase 2 (speech-to-text) prototype
3. **Next month:** Full speech + LLM pipeline
4. **Q2 2026:** Navigation and autonomous movement
5. **Q3 2026+:** Memory, personality, and deployment

See [PROJECT_GOALS.md](PROJECT_GOALS.md) for the complete roadmap and timeline.

---

**Happy building! Questions?** Check the [docs](README.md#-documentation) or [open an issue](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/issues).

