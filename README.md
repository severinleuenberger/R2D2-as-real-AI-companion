# R2D2 as a Real AI Companion

> Transform the iconic 1:2 DeAgostini R2-D2 into a fully autonomous AI companion robot with vision, voice, navigation, and personality‚Äîpowered by NVIDIA Jetson AGX Orin 64GB and ROS 2 Humble.

![R2D2 Full Robot](docs/photos/20251107_105518.jpg)

**Current Status:** Phase 1 Core System ‚Äî ~85% complete  
**Next Phase:** Speech & Language (STT + LLM + TTS)  
**Project Timeline:** 4 phases, 280-300 hours total (7 weeks full-time, 3-4 months part-time)

---

## üéØ Core Functional Requirements

The R2D2 project is designed to achieve the following functional capabilities:

### Communication & Conversation
- The robot must **listen to spoken input, convert speech to text, understand the user, generate a response, and speak it back** using text-to-speech
- The robot must **support natural, multi-turn conversation** without needing manual resets
- The robot must **allow direct verbal commands** such as "follow me," "go to the living room," "come here," or "look at this"
- The robot must **operate locally without requiring cloud services**, including offline STT, LLM, and TTS

### Perception & Recognition
- The robot must **recognize people, especially Severin, including detecting and identifying faces** through computer vision
- The robot must **track a person and optionally follow them** through the environment
- The robot must **recognize objects and obstacles** in front of it using camera and depth perception
- The robot must **detect rooms or general environment context** and understand where it is inside a home or office
- The robot must **react to events** in the environment, such as someone entering a room or calling its name
- The robot must **orient its camera** toward the person speaking or the object of interest

### Autonomous Navigation & Safety
- The robot must **autonomously navigate through indoor spaces, build a map, avoid obstacles, and move to requested locations**
- The robot must **maintain a safe driving speed and avoid collisions** with furniture, people, or pets
- The robot must **provide a heartbeat or "alive" signal** to indicate it is powered and operational

### Expression & Multi-Modal Interaction
- The robot must **respond with emotional or expressive sounds** similar to R2-D2 beeps
- The robot must **be able to play audio files**, including R2-D2 sound effects
- The robot must **combine perception, navigation, and conversation** so that it can interact socially and physically at the same time

---

## üöÄ Features (Current & Planned)

### Phase 1: Core System (Current) ‚úÖ
- [x] **Real-time Perception:** 30 FPS RGB camera stream, brightness metrics, face detection (90% accuracy)
- [x] **Face Recognition:** LBPH-based person identification (trained for multiple users)
- [x] **Hardware Integration:** OAK-D Lite depth camera, Jetson AGX Orin compute, ROS 2 infrastructure
- [x] **Professional Codebase:** Clean workspace structure, modular packages, parameter-driven configuration
- [x] **Comprehensive Docs:** Setup guides, integration patterns, operations checklist, architecture diagrams

### Phase 2: Speech & AI (Next) ‚è≥
- [ ] **Speech-to-Text:** Real-time audio input with wake-word detection ("Hey R2D2")
- [ ] **Language Model:** Local LLM inference (Llama 2 7B) for conversational responses
- [ ] **Text-to-Speech:** Real-time voice synthesis and playback
- [ ] **Context Awareness:** Vision-informed responses (greet by name, reference visual context)

### Phase 3: Navigation (Future) ‚è≥
- [ ] **SLAM Mapping:** Autonomous room mapping and localization
- [ ] **Autonomous Movement:** Differential drive control for 2-wheel locomotion
- [ ] **Obstacle Avoidance:** Real-time collision detection and path replanning
- [ ] **Room Navigation:** Go to named rooms, return to base, explore autonomously

### Phase 4: Memory & Personality (Future) ‚è≥
- [ ] **Conversation Memory:** Persistent history, context-aware responses
- [ ] **Learning & Adaptation:** Preference learning, anomaly detection
- [ ] **Expression:** LED animations, motor movements, tone variation
- [ ] **Multi-User Support:** Per-user profiles, personalized interactions

---

## üìñ Documentation

Comprehensive guides organized by audience and use case. **Start here:**

### For First-Time Users
1. **[Quick Start](#quick-start)** (below) ‚Äî Run the system in 5 minutes
2. **[PROJECT_GOALS.md](PROJECT_GOALS.md)** ‚Äî Understand the 4-phase roadmap and success metrics
3. **[ARCHITECTURE_OVERVIEW.md](ARCHITECTURE_OVERVIEW.md)** ‚Äî See how components fit together

### For Daily Operators
- **[OPERATIONS_CHECKLIST.md](OPERATIONS_CHECKLIST.md)** ‚Äî Startup, monitoring, troubleshooting, recovery procedures
- **[COMPUTE_COST_ANALYSIS.md](COMPUTE_COST_ANALYSIS.md)** ‚Äî CPU/memory usage, performance baselines, optimization tips

### For Phase 2-4 Developers
- **[000_INTERNAL_AGENT_NOTES.md](000_INTERNAL_AGENT_NOTES.md)** ‚Äî ARM architecture quirks, DepthAI setup, common issues and solutions
- **[INTEGRATION_GUIDE.md](INTEGRATION_GUIDE.md)** ‚Äî Step-by-step guide to add speech, navigation, memory systems

### Technical Depth (Phase 1 Subsystems)
| Component | Document | Purpose |
|-----------|----------|---------|
| **Foundations** | [000_INTERNAL_AGENT_NOTES.md](000_INTERNAL_AGENT_NOTES.md) | Critical git rules, environment setup, hardware constants |
| **Architecture** | [001_ARCHITECTURE_OVERVIEW.md](001_ARCHITECTURE_OVERVIEW.md) | System design, software stack, ROS 2 topics |
| **System Setup** | [010_PROJECT_GOALS_AND_SETUP.md](010_PROJECT_GOALS_AND_SETUP.md) | Jetson flashing, ROS 2 installation, workspace setup |
| **Camera Integration** | [020_CAMERA_SETUP_DOCUMENTATION.md](020_CAMERA_SETUP_DOCUMENTATION.md) | OAK-D Lite + DepthAI SDK, ROS 2 camera_node |
| **Image Processing** | [030_PERCEPTION_PIPELINE_SETUP.md](030_PERCEPTION_PIPELINE_SETUP.md) | Brightness metrics, Haar Cascade face detection, pipeline |
| **Face Recognition** | [040_FACE_RECOGNITION_COMPLETE.md](040_FACE_RECOGNITION_COMPLETE.md) | LBPH training, real-time recognition, model management |
| **Backup & Restore** | [004_BACKUP_AND_RESTORE.md](004_BACKUP_AND_RESTORE.md) | Full-system backup for reproducible deployments |

---

## ‚úÖ Phase 1 Status

**Completion:** ~85% (core systems operational, documentation polishing)

![R2D2 Empty Body](docs/photos/empty_body.jpg)

### What's Working
- ‚úÖ **Jetson Setup:** JetPack 6.x, ROS 2 Humble, clean workspace structure
- ‚úÖ **Camera:** OAK-D Lite streaming 30 FPS RGB (1920√ó1080) at `/oak/rgb/image_raw`
- ‚úÖ **Perception:** Brightness metrics + Haar Cascade face detection (13 Hz) + LBPH recognition (6.5 Hz)
- ‚úÖ **Node Architecture:** 4 ROS 2 packages with parameter-driven configuration
- ‚úÖ **Performance:** ~10-15% CPU usage (perception pipeline), excellent thermal stability
- ‚úÖ **Documentation:** 7+ technical guides + architecture diagrams + integration templates

### Remaining Phase 1 Tasks
- ‚è≥ **README Improvements** (this file ‚Äî making it more accessible)
- ‚è≥ **PROJECT_GOALS.md** (complete roadmap + 4-phase vision)
- ‚è≥ Git commit of final documentation

### Phase 1 Exit Criteria
When all above are done:
- ‚úÖ New developers can start camera stream in 5 minutes
- ‚úÖ Architecture is clear (blocks, data flow, integration points)
- ‚úÖ Existing documentation is audited and organized
- ‚úÖ Path to Phase 2 (Speech) is documented
- **‚Üí Ready to start Phase 2**

---

## üìä Project Status & Roadmap

### Overall Progress: 30% Complete (16 Core Objectives)

| Domain | Objective | Phase | Status | Notes |
|--------|-----------|-------|--------|-------|
| **Conversation** | Natural voice interaction | 2 | ‚è≥ Next | STT + LLM + TTS pipeline |
| **Conversation** | Multi-turn dialogue | 2 | ‚è≥ Next | Requires Phase 2 |
| **Conversation** | Local AI processing | 2 | ‚è≥ Next | Ollama + Llama 2 7B |
| **Perception** | Face recognition | 1 | ‚úÖ Done | LBPH trained, 85-92% accuracy |
| **Perception** | Person tracking | 3 | ‚è≥ Future | Requires navigation Phase |
| **Perception** | Object detection | 3 | ‚è≥ Future | Post-Phase 2 |
| **Perception** | Room understanding | 3 | ‚è≥ Future | SLAM + spatial mapping |
| **Perception** | Situation awareness | 1 | ‚úÖ Partial | Brightness + face count available |
| **Navigation** | Indoor navigation | 3 | ‚è≥ Future | Requires motor + Nav2 integration |
| **Navigation** | Mapping & localization | 3 | ‚è≥ Future | SLAM with OAK-D depth |
| **Navigation** | Obstacle avoidance | 3 | ‚è≥ Future | Real-time replanning |
| **Navigation** | Safe movement | 3 | ‚è≥ Future | Speed limiting + collision detection |
| **Expression** | Expressive audio | 2 | ‚è≥ Next | Beep synthesis + sound files |
| **Expression** | Social responsiveness | 2 | ‚è≥ Next | Multi-modal coordination |
| **Expression** | Directional awareness | 1 | ‚úÖ Partial | Camera orientation possible |
| **Commands** | Verbal command interface | 2 | ‚è≥ Next | After STT + LLM |

### Phase Breakdown

```
Phase 1: Core System Bringup (30% total project)
‚îú‚îÄ ‚úÖ Jetson + ROS 2 setup
‚îú‚îÄ ‚úÖ Camera integration (30 FPS RGB streaming)
‚îú‚îÄ ‚úÖ Perception pipeline (brightness + face detection/recognition)
‚îú‚îÄ ‚úÖ Professional documentation
‚îî‚îÄ üìç Current: ~85% complete

Phase 2: Speech & Language (35% total project)
‚îú‚îÄ ‚è≥ Audio hardware integration (ReSpeaker 2-Mic HAT)
‚îú‚îÄ ‚è≥ Speech-to-Text (Whisper or similar)
‚îú‚îÄ ‚è≥ Local LLM (Ollama + Llama 2 7B)
‚îú‚îÄ ‚è≥ Text-to-Speech (synthesis engine)
‚îú‚îÄ ‚è≥ Wake-word detection ("Hey R2D2")
‚îî‚îÄ üéØ Target: 6-8 weeks, complete by mid-January 2026

Phase 3: Navigation & Movement (20% total project)
‚îú‚îÄ Motor control (PWM drivers, encoders)
‚îú‚îÄ SLAM mapping (OAK-D depth + visual odometry)
‚îú‚îÄ Nav2 path planning
‚îú‚îÄ Obstacle avoidance
‚îî‚îÄ üéØ Target: 8-10 weeks after Phase 2

Phase 4: Memory & Personality (15% total project)
‚îú‚îÄ Conversation memory (SQLite, context management)
‚îú‚îÄ Learning & adaptation (reinforcement from feedback)
‚îú‚îÄ LED animations + motor expressions
‚îú‚îÄ Multi-user support
‚îî‚îÄ üéØ Target: 10+ weeks after Phase 3
```

---

## üîÑ Recommended Next Steps (Your Priorities)

Based on your goals to implement **mic & speaker integration ‚Üí STT/LLM/TTS**, here's the optimal order:

### Step 1: Audio Hardware Integration (Week 1-2)
**Goal:** Get microphone and speaker working with Jetson

**Tasks:**
1. ‚úÖ ReSpeaker 2-Mic HAT hardware setup (physical connection + power)
2. ‚è≥ ReSpeaker driver installation (SAI sound card)
3. ‚è≥ Audio input test (`arecord`, `aplay`)
4. ‚è≥ Create `r2d2_audio` ROS 2 package
5. ‚è≥ Audio node: publish raw audio frames to `/r2d2/audio/raw`
6. ‚è≥ Speaker node: subscribe to `/r2d2/audio/output` and play

**Deliverable:** 
- ROS 2 topics for microphone input and speaker output
- Test recording + playback cycle working
- Document in INTEGRATION_GUIDE

---

### Step 2: Speech-to-Text (Week 2-3)
**Goal:** Convert spoken audio to text in real-time

**Tasks:**
1. ‚è≥ Evaluate STT options:
   - Whisper (OpenAI, CPU-friendly, good accuracy)
   - Vosk (lightweight, offline, less accurate)
   - Google Speech Recognition (requires cloud - not preferred)
2. ‚è≥ Install selected STT engine in depthai_env
3. ‚è≥ Create `r2d2_speech_to_text` node
4. ‚è≥ Subscribe to `/r2d2/audio/raw`, output to `/r2d2/speech/text`
5. ‚è≥ Add wake-word detection ("Hey R2D2")
6. ‚è≥ Benchmark: latency, accuracy, CPU usage

**Deliverable:**
- Working STT node
- Wake-word detection triggering transcription
- <1 second latency from speech end to text output
- CPU usage metrics documented

---

### Step 3: Local Language Model (Week 3-5)
**Goal:** Generate contextual responses from text input

**Tasks:**
1. ‚è≥ Install Ollama (LLM inference framework)
2. ‚è≥ Download Llama 2 7B model (fits in Jetson AGX memory)
3. ‚è≥ Test inference speed (target: <2 sec for response)
4. ‚è≥ Create `r2d2_language_model` node
5. ‚è≥ Subscribe to `/r2d2/speech/text`
6. ‚è≥ Add context awareness (recent face recognition, brightness, etc.)
7. ‚è≥ Output to `/r2d2/ai/response`

**Deliverable:**
- LLM node generating contextual responses
- Integration with perception system (greet by name, reference environment)
- <2 second response time
- CPU/memory profiling

---

### Step 4: Text-to-Speech (Week 5-6)
**Goal:** Synthesize spoken responses with natural voice

**Tasks:**
1. ‚è≥ Evaluate TTS options:
   - pyttsx3 (lightweight, offline, multiple voices)
   - glow-tts (higher quality, more CPU)
   - espeak (minimal resources)
2. ‚è≥ Install and test selected TTS
3. ‚è≥ Create `r2d2_text_to_speech` node
4. ‚è≥ Subscribe to `/r2d2/ai/response`
5. ‚è≥ Generate audio frames to `/r2d2/audio/output`
6. ‚è≥ Benchmark: latency, naturalness, CPU usage

**Deliverable:**
- Full pipeline: speech ‚Üí text ‚Üí understanding ‚Üí response ‚Üí speech
- <3 second total latency (audio in to audio out)
- Usable voice quality

---

### Step 5: Full Integration & Testing (Week 6-8)
**Goal:** End-to-end conversational AI system

**Tasks:**
1. ‚è≥ Connect all nodes with proper message flow
2. ‚è≥ Add error handling (no speech, no response, etc.)
3. ‚è≥ Implement conversation memory (last 5 exchanges)
4. ‚è≥ Test with real use cases:
   - "Hello R2D2, what's your name?" 
   - "What do you see?" (use perception data)
   - "Who am I?" (face recognition)
   - Multi-turn dialogue
5. ‚è≥ Performance optimization:
   - Profile CPU/GPU/memory
   - Optimize model inference
   - Document resource usage
6. ‚è≥ Update documentation and examples

**Deliverable:**
- Complete Phase 2 system ready for Phase 3
- All components documented and tested
- Performance baseline: <3 sec end-to-end latency
- Ready to add navigation features

---

## üìà Expected Challenges & Solutions

| Challenge | Phase | Solution |
|-----------|-------|----------|
| ReSpeaker audio not detected | 2.1 | Check SAI kernel module load, verify USB connection |
| STT too slow on Jetson ARM | 2.2 | Use quantized models, reduce batch size, profile CPU |
| LLM memory overflow | 2.3 | Use 7B model max, enable 8-bit quantization |
| TTS latency > 2 sec | 2.4 | Pre-encode common responses, use faster model |
| Conversation feels unnatural | 2.5 | Add system prompt for R2D2 personality, context memory |

---

---

## üîß Hardware

The iconic R2-D2 is being rebuilt with modern robotics hardware and AI compute.

### Current Hardware Stack
| Component | Model | Purpose | Status |
|-----------|-------|---------|--------|
| **Chassis** | DeAgostini R2-D2 1:2 Kit | Main body (48 cm tall) | ‚úÖ Complete |
| **Compute** | NVIDIA Jetson AGX Orin 64GB | AI brain (12-core ARM, 504-GPU cores, 100W TDP) | ‚úÖ Mounted & Running |
| **Camera** | Luxonis OAK-D Lite Auto Focus | Vision (1920√ó1080 @ 30 FPS, depth + IMU) | ‚úÖ Integrated |
| **Audio Input** | ReSpeaker 2-Mic HAT | Voice capture (for Phase 2 STT) | ‚è≥ Ordered |
| **Drive** | DeAgostini DC Motors (2√ó) | Leg & dome motors with encoders | ‚è≥ Not yet integrated |
| **Motor Control** | Pololu MC33926 (2√ó) | H-bridge drivers for DC motors | ‚úÖ Assembled |
| **Power** | 4S LiPo 5000 mAh (14.8V) | Main battery system | ‚úÖ Charged & Ready |
| **Power Dist** | Custom DC-DC (14V‚Üí12V/5V) | Jetson + ReSpeaker + motors | ‚è≥ Not yet integrated |
| **Internal** | WS2812B RGB LEDs | Status & personality expression | ‚è≥ Coming in Phase 4 |

### Inside the Bot
![R2D2 Body With Jetson](docs/photos/body%20with%20jetson.jpg)  
The internal structure houses the Jetson AGX Orin, OAK-D camera, power distribution, and future motor drivers. Careful cable management ensures room for Phase 2-4 additions.

### Bill of Materials (Full Project)
See [BOM_HARDWARE.md](BOM_HARDWARE.md) for detailed part numbers, sourcing links, and cost breakdown (~$3,600 including chassis).



## Repository Structure

The repository reflects the active development environment on the Jetson AGX Orin.  
Generated ROS 2 build artifacts are excluded via `.gitignore`, resulting in a clean and minimal source tree.

```text
.
‚îú‚îÄ ros2_ws/
‚îÇ  ‚îú‚îÄ src/
‚îÇ  ‚îÇ  ‚îú‚îÄ r2d2_hello/      # First functional nodes (beep + heartbeat)
‚îÇ  ‚îÇ  ‚îî‚îÄ r2d2_bringup/    # Bringup launch to start the robot system
‚îÇ  ‚îú‚îÄ build/              # (generated, ignored)
‚îÇ  ‚îú‚îÄ install/            # (generated, ignored)
‚îÇ  ‚îî‚îÄ log/                # (generated, ignored)
‚îú‚îÄ docs/
‚îÇ  ‚îî‚îÄ photos/             # Build documentation images
‚îú‚îÄ tests/                 # GPU/Audio/Camera ‚Äútouch-the-ground‚Äù tests (planned)
‚îî‚îÄ scripts/               # Utility scripts
```



## üöÄ Quick Start

### 1. Clone & Setup (5 min)

```bash
# Clone repository
git clone git@github.com:severinleuenberger/R2D2-as-real-AI-companion.git
cd R2D2-as-real-AI-companion

# Set up environment (CRITICAL: order matters!)
source ~/depthai_env/bin/activate
export OPENBLAS_CORETYPE=ARMV8
source ~/.bashrc
cd ros2_ws
source install/setup.bash
```

### 2. Launch Camera & Perception (1 command)

```bash
ros2 launch r2d2_bringup r2d2_camera_perception.launch.py
```

**Expected Output (first 5 seconds):**
```
[INFO] [camera_node]: OAK-D camera initialized
[INFO] [image_listener]: ImageListener node initialized
[INFO] [image_listener]: Haar Cascade loaded successfully
[INFO] All nodes started successfully
```

### 3. Verify System (In another terminal, same env setup as above)

```bash
# Check camera frame rate (should be ~30 Hz)
ros2 topic hz /oak/rgb/image_raw

# Check perception output (should be ~13 Hz)
ros2 topic hz /r2d2/perception/brightness

# See brightness values (should be ~130-140 in normal light)
watch -n 0.5 'ros2 topic echo /r2d2/perception/brightness -n 1'

# See face detection (0 = no one, 1+ = detected)
ros2 topic echo /r2d2/perception/face_count
```

### 4. Troubleshooting

If things don't work:
1. Check [OPERATIONS_CHECKLIST.md](OPERATIONS_CHECKLIST.md) ‚Üí Section 5 (Troubleshooting)
2. Verify environment setup: `echo $OPENBLAS_CORETYPE` should print `ARMV8`
3. Check camera: `lsusb | grep Movidius`

**Time to working system:** ~7 seconds from launch command

---

## üìÅ Repository Structure

```
r2d2/
‚îú‚îÄ‚îÄ README.md                              # This file
‚îú‚îÄ‚îÄ 000_INTERNAL_AGENT_NOTES.md             # ARM quirks, DepthAI setup, performance baselines
‚îú‚îÄ‚îÄ 001_ARCHITECTURE_OVERVIEW.md            # System design, data flow, integration patterns
‚îú‚îÄ‚îÄ 002_HOW_TO_INSTRUCT_CLAUDE.md           # How to ask Claude for R2D2 task help
‚îú‚îÄ‚îÄ 003_JETSON_FLASHING_AND_DISPLAY_SETUP.md # Hardware setup procedures
‚îú‚îÄ‚îÄ 004_BACKUP_AND_RESTORE.md              # Backup/restore procedures
‚îú‚îÄ‚îÄ 010_PROJECT_GOALS_AND_SETUP.md         # 4-phase roadmap, success metrics
‚îú‚îÄ‚îÄ 020_CAMERA_SETUP_DOCUMENTATION.md      # OAK-D camera + DepthAI SDK
‚îú‚îÄ‚îÄ 030_PERCEPTION_PIPELINE_SETUP.md       # Image processing pipeline
‚îú‚îÄ‚îÄ 040_FACE_RECOGNITION_COMPLETE.md       # Face recognition system + training
‚îú‚îÄ‚îÄ 050_AUDIO_SETUP_AND_CONFIGURATION.md   # Audio system setup
‚îú‚îÄ‚îÄ 060_AUDIO_NOTIFICATIONS_ROS2_INTEGRATION.md # Audio integration
‚îú‚îÄ‚îÄ OPERATIONS_CHECKLIST.md                # Daily startup, monitoring, troubleshooting
‚îú‚îÄ‚îÄ INTEGRATION_GUIDE.md                   # How to add Phase 2-4 features (template + examples)
‚îú‚îÄ‚îÄ COMPUTE_COST_ANALYSIS.md               # Performance profiles
‚îÇ
‚îú‚îÄ‚îÄ ros2_ws/                               # ROS 2 Humble workspace
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ r2d2_camera/                  # Camera node
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ r2d2_perception/              # Perception node
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ r2d2_hello/                   # Heartbeat + beep
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ r2d2_bringup/                 # Launch files
‚îÇ   ‚îú‚îÄ‚îÄ build/ install/ log/              # (generated, gitignored)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ face_recognition/models/          # Trained LBPH models
‚îÇ
‚îú‚îÄ‚îÄ tests/                                 # Component test scripts
‚îú‚îÄ‚îÄ docs/photos/                           # Build progress photos
‚îî‚îÄ‚îÄ scripts/                               # Utility scripts
```

---

## üîó Bill of Materials (BOM)

See [PROJECT_GOALS.md](PROJECT_GOALS.md) ‚Üí "Resource Requirements" section for detailed hardware breakdown.

**Quick Summary:**
- **Total cost (compute + sensors):** ~$2,200
- **With DeAgostini kit:** ~$3,600
- **Key sources:** NVIDIA, Luxonis, HobbyKing, Pololu, Seeed Studio

---

## ü§ù Contributing & Community

**GitHub:** [severinleuenberger/R2D2-as-real-AI-companion](https://github.com/severinleuenberger/R2D2-as-real-AI-companion)

**Discussion:**
- [NVIDIA Developer Forums](https://forums.developer.nvidia.com/) (R2D2 thread)
- [GitHub Discussions](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/discussions)

**How to Contribute:**
- **Found a bug?** ‚Üí [Create an Issue](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/issues)
- **Have an improvement?** ‚Üí [Submit a Pull Request](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/pulls)
- **Want to discuss?** ‚Üí [Start a Discussion](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/discussions)
- **Building your own R2D2?** ‚Üí Document it and share!

**License:** [MIT](LICENSE) - Free to use, modify, and distribute

**Author:** [@s_leuenberger](https://x.com/s_leuenberger) | Switzerland

---

## üéØ What's Next?

1. **This week:** Complete Phase 1 documentation
2. **Next week:** Begin Phase 2 (speech-to-text) prototype
3. **Next month:** Full speech + LLM pipeline
4. **Q2 2026:** Navigation and autonomous movement
5. **Q3 2026+:** Memory, personality, and deployment

See [PROJECT_GOALS.md](PROJECT_GOALS.md) for the complete roadmap and timeline.

---

**Happy building! Questions?** Check the [docs](README.md#-documentation) or [open an issue](https://github.com/severinleuenberger/R2D2-as-real-AI-companion/issues).

