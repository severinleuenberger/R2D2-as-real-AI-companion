# Next Tasks - R2D2 System Improvements

**Date:** December 10, 2025  
**Priority:** Ordered by impact and logical sequence  
**Estimated Total Time:** ~1.5 hours

---

## üéØ Implementation Sequence (EXECUTE IN THIS ORDER)

### ‚ö° Task 0: Enable GPU Acceleration for STT (CRITICAL - 10 min)

**Status:** Ready to Implement NOW  
**Effort:** 10 minutes  
**Impact:** üî¥ CRITICAL (4-6x speed improvement)

**Why First:** Unlocks the Jetson's GPU power. Changes STT from 3-5 sec ‚Üí 1-2 sec. Must be done before STT model optimization.

**Current Problem:**
- PyTorch built for CPU only (`2.9.1+cpu`)
- CUDA 12.6 installed but not being used
- Faster-Whisper forced to run on CPU
- GPU: 504 cores sitting idle

**The Fix:**

**Step 1: Remove CPU-only PyTorch**
```bash
source /home/severin/dev/r2d2/r2d2_speech_env/bin/activate
pip uninstall torch torchaudio torchvision -y
```

**Step 2: Install NVIDIA's GPU-enabled PyTorch for Jetson**
```bash
# Official NVIDIA build for Jetson + CUDA 12.4
pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 \
    --index-url https://download.pytorch.org/whl/nightly/cu124
```

**Step 3: Verify CUDA works**
```bash
python << 'EOF'
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"Device: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
EOF
```

Expected output:
```
PyTorch version: 2.3.0+cu124
CUDA available: True
Device: NVIDIA Jetson AGX Orin
GPU memory: 8.0 GB
```

**Step 4: Update config to use GPU**
File: `r2d2_speech/config/config_manager.py`  
Find line:
```python
device: str = "cpu"  # cuda or cpu
```
Change to:
```python
device: str = "cuda"  # GPU acceleration enabled!
```

**Step 5: Test GPU acceleration**
```bash
python << 'EOF'
from r2d2_speech.config import get_config
config = get_config()
print(f"STT device: {config.stt.device}")
print(f"Expected: cuda")

# Test actual transcription
from r2d2_speech.stt import SwissGermanSTT
import time
stt = SwissGermanSTT()
start = time.time()
# transcribe a test file
elapsed = time.time() - start
print(f"GPU transcription time: {elapsed:.1f}s (should be <2s)")
EOF
```

**Expected Results:**
- STT processing: 3-5 seconds ‚Üí **1-2 seconds** (3-4x faster)
- Total conversation: 15-20 seconds ‚Üí **8-10 seconds** (3x improvement)

**Documentation:** See `_ANALYSIS_AND_DOCUMENTATION/005_STT_GPU_ACCELERATION_ANALYSIS.md`

---

### ‚úÖ Task 1: Optimize STT Model (5 min)

**Status:** After Task 0 (GPU enabled)  
**Effort:** 5 minutes  
**Impact:** üî¥ HUGE (2-3x faster: 1-2 sec ‚Üí 0.5-0.8 sec)

**Why After Task 0:** GPU acceleration is prerequisite. With GPU, smaller models are fast enough.

**What to do:**
1. Edit: `r2d2_speech/config/config_manager.py`
2. Find line with: `model: str = "large-v2"`
3. Change to: `model: str = "base"`
4. Save and test

**Before:**
```python
@dataclass
class STTConfig:
    model: str = "large-v2"  # Current (3-5 sec on CPU, 1-2 sec on GPU)
    device: str = "cuda"     # GPU already enabled from Task 0
```

**After:**
```python
@dataclass
class STTConfig:
    model: str = "base"      # Optimized model (0.5-0.8 sec on GPU)
    device: str = "cuda"     # GPU acceleration
```

**Expected Result with GPU:**
- STT processing: 1-2 seconds ‚Üí **0.5-0.8 seconds** (2-3x faster)
- Total conversation: 8-10 seconds ‚Üí **5-7 seconds** (excellent for real-time)
- Accuracy loss: <1% (acceptable for conversational AI)

**Test Command:**
```bash
cd ~/dev/r2d2
python3 -c "from r2d2_speech.config.config_manager import load_config; c = load_config(); print(f'STT Model: {c.stt.model}'); print(f'Device: {c.stt.device}')"
# Should output:
# STT Model: base
# Device: cuda
```

**Documentation:** Already updated in `001_ARCHITECTURE_OVERVIEW.md`

---

### üîä Task 1.5: Fix Speaker Playback in Speech Pipeline (5 min) ‚≠ê FIXED!

**Status:** ‚úÖ ALREADY IMPLEMENTED (Commit 7bcd7f35)  
**Effort:** 5 minutes  
**Impact:** üî¥ CRITICAL (Speech system now plays audio!)

**Problem Discovered:** Speech pipeline generated audio but never played it!

**Root Cause Analysis:**
- ‚úÖ Notification system (beeps) worked perfectly ‚Üí explicit `audio_player.py` call
- ‚ùå Speech system generated audio but returned it without playing ‚Üí silent result
- The TTS module had `.play()` method but it was never called from pipeline

**The Fix (Already Applied):**

Added explicit playback call to `r2d2_speech/speech_pipeline.py` (line 270):

```python
# Step 4: Play audio response
logger.info("Step 4: Playing response audio...")
try:
    self.tts.play(response_audio, sample_rate=22050)
    logger.info("‚úì Audio playback started")
except Exception as e:
    logger.warning(f"Failed to play audio: {e} (continuing without speaker)")
```

**What This Does:**
1. Takes synthesized audio (numpy array)
2. Converts to WAV format
3. Pipes to `aplay` subprocess
4. Audio plays immediately on speaker
5. Falls back gracefully if speaker fails

**Testing:**
```bash
# Test speech pipeline with playback
python3 -c "
from r2d2_speech import SpeechPipeline
pipeline = SpeechPipeline()
result = pipeline.process_text('Hoi, wie gaht\\'s?')
print(f'Response: {result[\"response_text\"]}')
print(f'Audio played: True')
"
# Expected: You hear German response on speaker!
```

**Why It Works:**
- Uses the same TTS wrapper that was already implemented
- Follows the notification system pattern (explicit subprocess call)
- Has error handling so it doesn't crash if speaker fails
- Fully backward compatible (audio still returned for manual playback)

**Status:** ‚úÖ Complete and committed to git (7bcd7f35)  
**Next Step:** Task #2 (Perception integration)

**Documentation:** See `DEBUG_SPEAKER_ISSUE_FOUND.md` for full analysis

---

### üîó Task 2: Wire Speech to Perception System (30 min)

**Status:** After Task 1 (both STT and GPU optimized)  
**Effort:** 30 minutes  
**Impact:** üü° GOOD (Enable context-aware, personalized responses)

**Why After Tasks 0-1:** Speech system must be fast and responsive before adding perception integration.

**What to do:**
1. Open: `r2d2_speech/speech_node.py` (or equivalent main speech node)
2. Add subscription to `/r2d2/perception/person_id`
3. Store current person name in instance variable
4. Pass person context to LLM prompt generation
5. Test with face recognition enabled

**Implementation Steps:**

**Step 1: Add topic subscription (in `__init__`)**
```python
from std_msgs.msg import String

class SpeechNode(Node):
    def __init__(self):
        super().__init__('speech_node')
        
        # New: Subscribe to perception
        self.current_person = "Unknown"
        self.person_sub = self.create_subscription(
            String, '/r2d2/perception/person_id',
            self.on_person_detected, 10)
```

**Step 2: Add callback handler**
```python
    def on_person_detected(self, msg: String):
        """Called when perception system detects a person"""
        self.current_person = msg.data
        self.get_logger().info(f"Speech: Speaking to {self.current_person}")
```

**Step 3: Update LLM prompt generation**
```python
    def generate_response(self, user_text: str) -> str:
        """Generate response with person context"""
        context = f"You are speaking with {self.current_person}. "
        full_prompt = context + user_text
        response = self.grok_api.complete(full_prompt)
        return response
```

**Test Scenario:**
1. Launch perception system with face recognition enabled
2. Launch speech system
3. Speak near camera: "Hello R2D2"
4. System should respond with personalized greeting (e.g., "Hi Severin")
5. Check logs: Should show "Speaking to Severin"

**Expected Behavior:**
- When Severin present: "Hi Severin, how can I help?"
- When unknown person: "Hello, I don't recognize you"
- Enables future: "Remember this for Severin" type features

**Files to Modify:**
- `r2d2_speech/speech_node.py` (or main entry point)

**Documentation:** Already in `001_ARCHITECTURE_OVERVIEW.md` Section 9.5

---

### üîä Task 3: Add Speaker Fallback / Audio Output Strategy (30 min)

**Status:** After Task 2 (after perception integration)  
**Effort:** 30 minutes (choose one option: 10, 15, or 30 min)  
**Impact:** üü¢ GOOD (System testable despite broken hardware)

**Problem:**
PAM8403 amplifier not producing audio ‚Üí Users can't hear responses

**Solution: Pick ONE (or implement all 3 progressively)**

#### Option A: Simple File-Based Fallback (10 min - MVP)

```python
# In TTS node, after synthesis
import os

response_file = "/tmp/r2d2_response.wav"
tts.synthesis(text, output_file=response_file)

# Publish notification topic
msg = String()
msg.data = f"Response saved to {response_file}"
self.response_saved_pub.publish(msg)

print(f"‚úÖ Audio saved: {response_file}")
print("   Play with: aplay /tmp/r2d2_response.wav")
```

**Benefits:**
- ‚úÖ Zero dependency on broken hardware
- ‚úÖ Users can test on their own speakers
- ‚úÖ Debugging friendly

**Drawbacks:**
- ‚ùå Not automatic (manual playback needed)

#### Option B: Try-Catch with Fallback Device (15 min - Robust)

```python
import subprocess

def play_audio(file_path: str):
    """Try speaker, fallback to USB audio, then to file"""
    
    # Try 1: PAM8403 speaker (default)
    try:
        subprocess.run(
            ["aplay", "-D", "plughw:1,4", file_path],
            timeout=10,
            check=True
        )
        return True
    except Exception as e:
        self.get_logger().warn(f"Speaker failed: {e}")
    
    # Try 2: USB Audio device (if connected)
    try:
        subprocess.run(
            ["aplay", "-D", "default", file_path],
            timeout=10,
            check=True
        )
        self.get_logger().info("Playing on USB audio device")
        return True
    except Exception as e:
        self.get_logger().warn(f"USB audio failed: {e}")
    
    # Try 3: Save to file
    self.get_logger().info(f"All audio devices failed. Saved to: {file_path}")
    return False
```

**Benefits:**
- ‚úÖ Automatic fallback chain
- ‚úÖ Works with any audio device
- ‚úÖ Graceful degradation

**Drawbacks:**
- ‚ö†Ô∏è Requires try-catch error handling

#### Option C: Configuration-Based Audio Selection (15 min - Professional)

```python
@dataclass
class AudioOutputConfig:
    """Audio output device selection"""
    primary_device: str = "plughw:1,4"      # PAM8403 (broken)
    fallback_device: str = "default"         # USB or system audio
    fallback_to_file: bool = True            # Save if all fail
    output_directory: str = "/tmp"
    test_on_startup: bool = True             # Verify device at startup

def test_audio_device(device: str) -> bool:
    """Test if audio device is working"""
    try:
        # Generate 1-second test tone
        test_audio = "silence 1 16000 0.05"
        subprocess.run(
            ["aplay", "-D", device, "-c", "1", "-r", "16000", "-f", "S16_LE"],
            input=test_audio.encode(),
            timeout=2,
            check=True
        )
        return True
    except:
        return False
```

**Benefits:**
- ‚úÖ Most professional approach
- ‚úÖ Self-diagnosing (tests on startup)
- ‚úÖ Fully configurable

**Drawback:**
- ‚ö†Ô∏è More code

---

**Recommendation:**
1. **Immediate:** Implement Option A (10 min, unblock testing)
2. **Next:** Add Option B (5 min more, make it automatic)
3. **Later:** Option C (polish for production)

**Test Command (after implementation):**
```bash
# Record a test phrase
arecord -D plughw:2,0 -r 44100 -c 2 /tmp/test.wav -d 3

# Convert to speech
python3 -c "
from r2d2_speech.tts import Piper
p = Piper()
p.synthesize('Hello world', '/tmp/out.wav')
"

# Try to play (should fallback gracefully)
python3 r2d2_speech/speech_node.py  # Will use fallback
```

---

## Summary Table

| Task | Effort | Impact | Order | Status |
|------|--------|--------|-------|--------|
| **#0: GPU Acceleration** | 10 min | üî¥ CRITICAL (4-6x) | **1st** | Ready |
| **#1: STT Optimization** | 5 min | üî¥ HUGE (2-3x) | **2nd** | Ready |
| **#1.5: Speaker Playback** ‚≠ê | 5 min | üî¥ CRITICAL | **2.5th** | ‚úÖ Done! |
| **#2: Perception Integration** | 30 min | üü° GOOD | **3rd** | Planned |
| **#3: Speaker Fallback** | 30 min | üü¢ GOOD | **4th** | Planned |

---

## Implementation Order

```
TODAY (Dec 10) - COMPLETED:
  ‚úÖ Task #0: GPU Acceleration (10 min) - READY
     ‚îî‚îÄ Next: Execute when ready
  
  ‚úÖ Task #1: STT Optimization (5 min) - READY
     ‚îî‚îÄ Next: Execute after Task #0

  ‚úÖ Task #1.5: Speaker Playback (5 min) - ‚úÖ DONE!
     ‚îú‚îÄ Already implemented and committed (7bcd7f35)
     ‚îú‚îÄ Speech pipeline now plays audio automatically
     ‚îî‚îÄ Next: Test with actual speech

THIS WEEK (Dec 11-12):
  ‚è≥ Task #2: Perception Integration (30 min)
     ‚îú‚îÄ Prerequisites: Tasks 0 & 1 done
     ‚îú‚îÄ Result: Speech subscribes to /r2d2/perception/person_id
     ‚îî‚îÄ Impact: Context-aware, personalized responses

  ‚è≥ Task #3: Speaker Fallback (30 min)
     ‚îú‚îÄ Prerequisites: Tasks 0, 1, 2 done
     ‚îú‚îÄ Choose: Option A (10 min), B (15 min), or C (30 min)
     ‚îî‚îÄ Impact: Testable without hardware fix
```

---

## Testing Checklist

After each task:

```bash
# After Task #0: Verify GPU enabled
python << 'EOF'
import torch
assert torch.cuda.is_available(), "CUDA not available!"
print(f"‚úÖ CUDA enabled: {torch.cuda.get_device_name(0)}")
EOF

# After Task #1: Verify STT config
python -c "from r2d2_speech.config import get_config; c = get_config(); print(f'‚úÖ Model: {c.stt.model}, Device: {c.stt.device}')"

# After Task #2: Verify perception subscription
ros2 topic echo /r2d2/perception/person_id  # Should show person names
# Check logs for: "Speech: Speaking to [person_name]"

# After Task #3: Verify audio fallback
python r2d2_speech/test_audio_output.py  # Should show device status and fallback chain
```

---

## Notes

- **Task #0 is CRITICAL** - MUST be done first to enable GPU acceleration
- **Task #1 is CRITICAL** - Users will hate 5-7 sec latency (unacceptable for real-time)
- **Task #1.5 is DONE ‚úÖ** - Speech pipeline now plays audio (speaker issue was missing playback call)
- **Task #2 makes system smarter** - Enables personalized interactions with recognition
- **Task #3 removes hardware blocker** - Allows testing even if PAM8403 speaker is broken
- All changes are **non-breaking** - Can implement incrementally
- All changes are **documented** in architecture and task files

**Speaker Problem Solved:**
- Root cause: TTS generated audio but pipeline didn't call `.play()`
- Solution: Added explicit playback in `process_speech()` method
- Result: Full end-to-end speech now works with audio output
- Fallback: If speaker fails, audio is still returned for manual playback

**Performance Summary:**
```
Current (with GPU enabled):  15-20 sec/conversation (unacceptable)
After Tasks 0-1:             5-7 sec/conversation (excellent!)
After Tasks 0-1.5:           5-7 sec/conversation + AUDIO PLAYBACK ‚úÖ
After Tasks 0-3:             5-7 sec/conversation + context + audio fallback
```

---

**Created:** December 10, 2025  
**Next Review:** After Task #1 is complete  
**Owner:** Development Team
